{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12437110,"sourceType":"datasetVersion","datasetId":7845121},{"sourceId":12437327,"sourceType":"datasetVersion","datasetId":7845269}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nimport os\n\n# Load all CSVs từ 2019 đến 2025\nall_files = sorted(glob.glob(\"/kaggle/input/energy-dataset/*.xlsx\"))\ndf_list = [pd.read_excel(file) for file in all_files]\npre_df = pd.concat(df_list, ignore_index=True)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:45:46.489196Z","iopub.execute_input":"2025-08-07T17:45:46.489415Z","iopub.status.idle":"2025-08-07T17:50:07.419497Z","shell.execute_reply.started":"2025-08-07T17:45:46.489398Z","shell.execute_reply":"2025-08-07T17:50:07.418688Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n  warn(\"Workbook contains no default style, apply openpyxl's default\")\n/usr/local/lib/python3.11/dist-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n  warn(\"Workbook contains no default style, apply openpyxl's default\")\n/usr/local/lib/python3.11/dist-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n  warn(\"Workbook contains no default style, apply openpyxl's default\")\n/usr/local/lib/python3.11/dist-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n  warn(\"Workbook contains no default style, apply openpyxl's default\")\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/preprocessing-dataset/preprocessed_energy_data_AT_2019_2025.csv\")\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:07.420413Z","iopub.execute_input":"2025-08-07T17:50:07.420821Z","iopub.status.idle":"2025-08-07T17:50:07.594068Z","shell.execute_reply.started":"2025-08-07T17:50:07.420793Z","shell.execute_reply":"2025-08-07T17:50:07.593312Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                   DateUTC                 MeasureItem   DateShort  \\\n0      2019-01-01 00:00:00  Monthly Hourly Load Values  2019-01-01   \n1      2019-01-01 01:00:00  Monthly Hourly Load Values  2019-01-01   \n2      2019-01-01 02:00:00  Monthly Hourly Load Values  2019-01-01   \n3      2019-01-01 03:00:00  Monthly Hourly Load Values  2019-01-01   \n4      2019-01-01 04:00:00  Monthly Hourly Load Values  2019-01-01   \n...                    ...                         ...         ...   \n54763  2025-03-31 19:00:00  Monthly Hourly Load Values  2025-03-31   \n54764  2025-03-31 20:00:00  Monthly Hourly Load Values  2025-03-31   \n54765  2025-03-31 21:00:00  Monthly Hourly Load Values  2025-03-31   \n54766  2025-03-31 22:00:00  Monthly Hourly Load Values  2025-03-31   \n54767  2025-03-31 23:00:00  Monthly Hourly Load Values  2025-03-31   \n\n                  TimeFrom               TimeTo CountryCode  Cov_ratio  \\\n0      1970-01-01 00:00:00  1970-01-01 01:00:00          AT        100   \n1      1970-01-01 01:00:00  1970-01-01 02:00:00          AT        100   \n2      1970-01-01 02:00:00  1970-01-01 03:00:00          AT        100   \n3      1970-01-01 03:00:00  1970-01-01 04:00:00          AT        100   \n4      1970-01-01 04:00:00  1970-01-01 05:00:00          AT        100   \n...                    ...                  ...         ...        ...   \n54763  1970-01-01 19:00:00  1970-01-01 20:00:00          AT        100   \n54764  1970-01-01 20:00:00  1970-01-01 21:00:00          AT        100   \n54765  1970-01-01 21:00:00  1970-01-01 22:00:00          AT        100   \n54766  1970-01-01 22:00:00  1970-01-01 23:00:00          AT        100   \n54767  1970-01-01 23:00:00  1970-01-01 00:00:00          AT        100   \n\n        Value  Value_ScaleTo100  hour  dayofweek  month  year  \n0      5852.5            5852.5     0          1      1  2019  \n1      5619.2            5619.2     1          1      1  2019  \n2      5323.9            5323.9     2          1      1  2019  \n3      5273.3            5273.3     3          1      1  2019  \n4      5439.0            5439.0     4          1      1  2019  \n...       ...               ...   ...        ...    ...   ...  \n54763  7196.0            7196.0    19          0      3  2025  \n54764  6896.8            6896.8    20          0      3  2025  \n54765  6434.1            6434.1    21          0      3  2025  \n54766  6102.1            6102.1    22          0      3  2025  \n54767  5840.9            5840.9    23          0      3  2025  \n\n[54768 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DateUTC</th>\n      <th>MeasureItem</th>\n      <th>DateShort</th>\n      <th>TimeFrom</th>\n      <th>TimeTo</th>\n      <th>CountryCode</th>\n      <th>Cov_ratio</th>\n      <th>Value</th>\n      <th>Value_ScaleTo100</th>\n      <th>hour</th>\n      <th>dayofweek</th>\n      <th>month</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-01-01 00:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 00:00:00</td>\n      <td>1970-01-01 01:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>5852.5</td>\n      <td>5852.5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-01-01 01:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 01:00:00</td>\n      <td>1970-01-01 02:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>5619.2</td>\n      <td>5619.2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-01-01 02:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 02:00:00</td>\n      <td>1970-01-01 03:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>5323.9</td>\n      <td>5323.9</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-01-01 03:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 03:00:00</td>\n      <td>1970-01-01 04:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>5273.3</td>\n      <td>5273.3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-01-01 04:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 04:00:00</td>\n      <td>1970-01-01 05:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>5439.0</td>\n      <td>5439.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>54763</th>\n      <td>2025-03-31 19:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 19:00:00</td>\n      <td>1970-01-01 20:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>7196.0</td>\n      <td>7196.0</td>\n      <td>19</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2025</td>\n    </tr>\n    <tr>\n      <th>54764</th>\n      <td>2025-03-31 20:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 20:00:00</td>\n      <td>1970-01-01 21:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>6896.8</td>\n      <td>6896.8</td>\n      <td>20</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2025</td>\n    </tr>\n    <tr>\n      <th>54765</th>\n      <td>2025-03-31 21:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 21:00:00</td>\n      <td>1970-01-01 22:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>6434.1</td>\n      <td>6434.1</td>\n      <td>21</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2025</td>\n    </tr>\n    <tr>\n      <th>54766</th>\n      <td>2025-03-31 22:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 22:00:00</td>\n      <td>1970-01-01 23:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>6102.1</td>\n      <td>6102.1</td>\n      <td>22</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2025</td>\n    </tr>\n    <tr>\n      <th>54767</th>\n      <td>2025-03-31 23:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 23:00:00</td>\n      <td>1970-01-01 00:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>5840.9</td>\n      <td>5840.9</td>\n      <td>23</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2025</td>\n    </tr>\n  </tbody>\n</table>\n<p>54768 rows × 13 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"pre_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:07.594753Z","iopub.execute_input":"2025-08-07T17:50:07.594972Z","iopub.status.idle":"2025-08-07T17:50:07.609192Z","shell.execute_reply.started":"2025-08-07T17:50:07.594917Z","shell.execute_reply":"2025-08-07T17:50:07.608625Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                        MeasureItem             DateUTC  DateShort  \\\n0        Monthly Hourly Load Values 2019-01-01 00:00:00 2019-01-01   \n1        Monthly Hourly Load Values 2019-01-01 01:00:00 2019-01-01   \n2        Monthly Hourly Load Values 2019-01-01 02:00:00 2019-01-01   \n3        Monthly Hourly Load Values 2019-01-01 03:00:00 2019-01-01   \n4        Monthly Hourly Load Values 2019-01-01 04:00:00 2019-01-01   \n...                             ...                 ...        ...   \n1909419  Monthly Hourly Load Values 2025-03-31 19:00:00 2025-03-31   \n1909420  Monthly Hourly Load Values 2025-03-31 20:00:00 2025-03-31   \n1909421  Monthly Hourly Load Values 2025-03-31 21:00:00 2025-03-31   \n1909422  Monthly Hourly Load Values 2025-03-31 22:00:00 2025-03-31   \n1909423  Monthly Hourly Load Values 2025-03-31 23:00:00 2025-03-31   \n\n                    TimeFrom               TimeTo CountryCode  Cov_ratio  \\\n0        1970-01-01 00:00:00  1970-01-01 01:00:00          AT        100   \n1        1970-01-01 01:00:00  1970-01-01 02:00:00          AT        100   \n2        1970-01-01 02:00:00  1970-01-01 03:00:00          AT        100   \n3        1970-01-01 03:00:00  1970-01-01 04:00:00          AT        100   \n4        1970-01-01 04:00:00  1970-01-01 05:00:00          AT        100   \n...                      ...                  ...         ...        ...   \n1909419  1970-01-01 19:00:00  1970-01-01 20:00:00          XK        100   \n1909420  1970-01-01 20:00:00  1970-01-01 21:00:00          XK        100   \n1909421  1970-01-01 21:00:00  1970-01-01 22:00:00          XK        100   \n1909422  1970-01-01 22:00:00  1970-01-01 23:00:00          XK        100   \n1909423  1970-01-01 23:00:00  1970-01-01 00:00:00          XK        100   \n\n           Value  Value_ScaleTo100              CreateDate  \\\n0        5852.50           5852.50 2024-05-29 11:37:27.260   \n1        5619.20           5619.20 2024-05-29 11:37:27.260   \n2        5323.90           5323.90 2024-05-29 11:37:27.260   \n3        5273.30           5273.30 2024-05-29 11:37:27.260   \n4        5439.00           5439.00 2024-05-29 11:37:27.260   \n...          ...               ...                     ...   \n1909419   863.25            863.25 2025-06-04 09:35:31.480   \n1909420   876.63            876.63 2025-06-04 09:35:31.480   \n1909421   879.89            879.89 2025-06-04 09:35:31.480   \n1909422   796.14            796.14 2025-06-04 09:35:31.480   \n1909423   655.07            655.07 2025-06-04 09:35:31.480   \n\n                     UpdateDate  \n0       2024-05-29 11:37:27.260  \n1       2024-05-29 11:37:27.260  \n2       2024-05-29 11:37:27.260  \n3       2024-05-29 11:37:27.260  \n4       2024-05-29 11:37:27.260  \n...                         ...  \n1909419 2025-06-04 09:35:31.480  \n1909420 2025-06-04 09:35:31.480  \n1909421 2025-06-04 09:35:31.480  \n1909422 2025-06-04 09:35:31.480  \n1909423 2025-06-04 09:35:31.480  \n\n[1909424 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MeasureItem</th>\n      <th>DateUTC</th>\n      <th>DateShort</th>\n      <th>TimeFrom</th>\n      <th>TimeTo</th>\n      <th>CountryCode</th>\n      <th>Cov_ratio</th>\n      <th>Value</th>\n      <th>Value_ScaleTo100</th>\n      <th>CreateDate</th>\n      <th>UpdateDate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01 00:00:00</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 00:00:00</td>\n      <td>1970-01-01 01:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>5852.50</td>\n      <td>5852.50</td>\n      <td>2024-05-29 11:37:27.260</td>\n      <td>2024-05-29 11:37:27.260</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01 01:00:00</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 01:00:00</td>\n      <td>1970-01-01 02:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>5619.20</td>\n      <td>5619.20</td>\n      <td>2024-05-29 11:37:27.260</td>\n      <td>2024-05-29 11:37:27.260</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01 02:00:00</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 02:00:00</td>\n      <td>1970-01-01 03:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>5323.90</td>\n      <td>5323.90</td>\n      <td>2024-05-29 11:37:27.260</td>\n      <td>2024-05-29 11:37:27.260</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01 03:00:00</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 03:00:00</td>\n      <td>1970-01-01 04:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>5273.30</td>\n      <td>5273.30</td>\n      <td>2024-05-29 11:37:27.260</td>\n      <td>2024-05-29 11:37:27.260</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01 04:00:00</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 04:00:00</td>\n      <td>1970-01-01 05:00:00</td>\n      <td>AT</td>\n      <td>100</td>\n      <td>5439.00</td>\n      <td>5439.00</td>\n      <td>2024-05-29 11:37:27.260</td>\n      <td>2024-05-29 11:37:27.260</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1909419</th>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31 19:00:00</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 19:00:00</td>\n      <td>1970-01-01 20:00:00</td>\n      <td>XK</td>\n      <td>100</td>\n      <td>863.25</td>\n      <td>863.25</td>\n      <td>2025-06-04 09:35:31.480</td>\n      <td>2025-06-04 09:35:31.480</td>\n    </tr>\n    <tr>\n      <th>1909420</th>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31 20:00:00</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 20:00:00</td>\n      <td>1970-01-01 21:00:00</td>\n      <td>XK</td>\n      <td>100</td>\n      <td>876.63</td>\n      <td>876.63</td>\n      <td>2025-06-04 09:35:31.480</td>\n      <td>2025-06-04 09:35:31.480</td>\n    </tr>\n    <tr>\n      <th>1909421</th>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31 21:00:00</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 21:00:00</td>\n      <td>1970-01-01 22:00:00</td>\n      <td>XK</td>\n      <td>100</td>\n      <td>879.89</td>\n      <td>879.89</td>\n      <td>2025-06-04 09:35:31.480</td>\n      <td>2025-06-04 09:35:31.480</td>\n    </tr>\n    <tr>\n      <th>1909422</th>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31 22:00:00</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 22:00:00</td>\n      <td>1970-01-01 23:00:00</td>\n      <td>XK</td>\n      <td>100</td>\n      <td>796.14</td>\n      <td>796.14</td>\n      <td>2025-06-04 09:35:31.480</td>\n      <td>2025-06-04 09:35:31.480</td>\n    </tr>\n    <tr>\n      <th>1909423</th>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31 23:00:00</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 23:00:00</td>\n      <td>1970-01-01 00:00:00</td>\n      <td>XK</td>\n      <td>100</td>\n      <td>655.07</td>\n      <td>655.07</td>\n      <td>2025-06-04 09:35:31.480</td>\n      <td>2025-06-04 09:35:31.480</td>\n    </tr>\n  </tbody>\n</table>\n<p>1909424 rows × 11 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Copy lại df để tránh mất dữ liệu gốc\ndf_fr = pre_df[pre_df[\"CountryCode\"] == \"FR\"].copy()\n\n# Đảm bảo DateUTC là kiểu datetime\ndf_fr['DateUTC'] = pd.to_datetime(df_fr['DateUTC'])\n\n# Tạo các feature mới\ndf_fr['hour'] = df_fr['DateUTC'].dt.hour\ndf_fr['dayofweek'] = df_fr['DateUTC'].dt.dayofweek\ndf_fr['month'] = df_fr['DateUTC'].dt.month\ndf_fr['year'] = df_fr['DateUTC'].dt.year\n\n# Sắp xếp theo thời gian để đảm bảo đúng thứ tự thời gian\ndf_fr = df_fr.sort_values(\"DateUTC\").reset_index(drop=True)\n\n# Đảm bảo các cột đúng thứ tự và giống như dữ liệu AT đã preprocessed\ndf_fr = df_fr[[\n    'DateUTC', 'MeasureItem', 'DateShort', 'TimeFrom', 'TimeTo',\n    'CountryCode', 'Cov_ratio', 'Value', 'Value_ScaleTo100',\n    'hour', 'dayofweek', 'month', 'year'\n]]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:07.611518Z","iopub.execute_input":"2025-08-07T17:50:07.611781Z","iopub.status.idle":"2025-08-07T17:50:07.786843Z","shell.execute_reply.started":"2025-08-07T17:50:07.611765Z","shell.execute_reply":"2025-08-07T17:50:07.786052Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df_fr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:07.787557Z","iopub.execute_input":"2025-08-07T17:50:07.787799Z","iopub.status.idle":"2025-08-07T17:50:07.800887Z","shell.execute_reply.started":"2025-08-07T17:50:07.787777Z","shell.execute_reply":"2025-08-07T17:50:07.800324Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                  DateUTC                 MeasureItem  DateShort  \\\n0     2019-01-01 00:00:00  Monthly Hourly Load Values 2019-01-01   \n1     2019-01-01 01:00:00  Monthly Hourly Load Values 2019-01-01   \n2     2019-01-01 02:00:00  Monthly Hourly Load Values 2019-01-01   \n3     2019-01-01 03:00:00  Monthly Hourly Load Values 2019-01-01   \n4     2019-01-01 04:00:00  Monthly Hourly Load Values 2019-01-01   \n...                   ...                         ...        ...   \n54697 2025-03-31 19:00:00  Monthly Hourly Load Values 2025-03-31   \n54698 2025-03-31 20:00:00  Monthly Hourly Load Values 2025-03-31   \n54699 2025-03-31 21:00:00  Monthly Hourly Load Values 2025-03-31   \n54700 2025-03-31 22:00:00  Monthly Hourly Load Values 2025-03-31   \n54701 2025-03-31 23:00:00  Monthly Hourly Load Values 2025-03-31   \n\n                  TimeFrom               TimeTo CountryCode  Cov_ratio  \\\n0      1970-01-01 00:00:00  1970-01-01 01:00:00          FR        100   \n1      1970-01-01 01:00:00  1970-01-01 02:00:00          FR        100   \n2      1970-01-01 02:00:00  1970-01-01 03:00:00          FR        100   \n3      1970-01-01 03:00:00  1970-01-01 04:00:00          FR        100   \n4      1970-01-01 04:00:00  1970-01-01 05:00:00          FR        100   \n...                    ...                  ...         ...        ...   \n54697  1970-01-01 19:00:00  1970-01-01 20:00:00          FR        100   \n54698  1970-01-01 20:00:00  1970-01-01 21:00:00          FR        100   \n54699  1970-01-01 21:00:00  1970-01-01 22:00:00          FR        100   \n54700  1970-01-01 22:00:00  1970-01-01 23:00:00          FR        100   \n54701  1970-01-01 23:00:00  1970-01-01 00:00:00          FR        100   \n\n            Value  Value_ScaleTo100  hour  dayofweek  month  year  \n0      60301.0000        60301.0000     0          1      1  2019  \n1      58540.0000        58540.0000     1          1      1  2019  \n2      55144.0000        55144.0000     2          1      1  2019  \n3      52978.0000        52978.0000     3          1      1  2019  \n4      52584.0000        52584.0000     4          1      1  2019  \n...           ...               ...   ...        ...    ...   ...  \n54697  49461.2200        49461.2200    19          0      3  2025  \n54698  50048.1900        50048.1900    20          0      3  2025  \n54699  49323.4600        49323.4600    21          0      3  2025  \n54700  46996.2300        46996.2300    22          0      3  2025  \n54701  45501.0625        45501.0625    23          0      3  2025  \n\n[54702 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DateUTC</th>\n      <th>MeasureItem</th>\n      <th>DateShort</th>\n      <th>TimeFrom</th>\n      <th>TimeTo</th>\n      <th>CountryCode</th>\n      <th>Cov_ratio</th>\n      <th>Value</th>\n      <th>Value_ScaleTo100</th>\n      <th>hour</th>\n      <th>dayofweek</th>\n      <th>month</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-01-01 00:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 00:00:00</td>\n      <td>1970-01-01 01:00:00</td>\n      <td>FR</td>\n      <td>100</td>\n      <td>60301.0000</td>\n      <td>60301.0000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-01-01 01:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 01:00:00</td>\n      <td>1970-01-01 02:00:00</td>\n      <td>FR</td>\n      <td>100</td>\n      <td>58540.0000</td>\n      <td>58540.0000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-01-01 02:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 02:00:00</td>\n      <td>1970-01-01 03:00:00</td>\n      <td>FR</td>\n      <td>100</td>\n      <td>55144.0000</td>\n      <td>55144.0000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2019-01-01 03:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 03:00:00</td>\n      <td>1970-01-01 04:00:00</td>\n      <td>FR</td>\n      <td>100</td>\n      <td>52978.0000</td>\n      <td>52978.0000</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2019-01-01 04:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2019-01-01</td>\n      <td>1970-01-01 04:00:00</td>\n      <td>1970-01-01 05:00:00</td>\n      <td>FR</td>\n      <td>100</td>\n      <td>52584.0000</td>\n      <td>52584.0000</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>54697</th>\n      <td>2025-03-31 19:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 19:00:00</td>\n      <td>1970-01-01 20:00:00</td>\n      <td>FR</td>\n      <td>100</td>\n      <td>49461.2200</td>\n      <td>49461.2200</td>\n      <td>19</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2025</td>\n    </tr>\n    <tr>\n      <th>54698</th>\n      <td>2025-03-31 20:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 20:00:00</td>\n      <td>1970-01-01 21:00:00</td>\n      <td>FR</td>\n      <td>100</td>\n      <td>50048.1900</td>\n      <td>50048.1900</td>\n      <td>20</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2025</td>\n    </tr>\n    <tr>\n      <th>54699</th>\n      <td>2025-03-31 21:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 21:00:00</td>\n      <td>1970-01-01 22:00:00</td>\n      <td>FR</td>\n      <td>100</td>\n      <td>49323.4600</td>\n      <td>49323.4600</td>\n      <td>21</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2025</td>\n    </tr>\n    <tr>\n      <th>54700</th>\n      <td>2025-03-31 22:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 22:00:00</td>\n      <td>1970-01-01 23:00:00</td>\n      <td>FR</td>\n      <td>100</td>\n      <td>46996.2300</td>\n      <td>46996.2300</td>\n      <td>22</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2025</td>\n    </tr>\n    <tr>\n      <th>54701</th>\n      <td>2025-03-31 23:00:00</td>\n      <td>Monthly Hourly Load Values</td>\n      <td>2025-03-31</td>\n      <td>1970-01-01 23:00:00</td>\n      <td>1970-01-01 00:00:00</td>\n      <td>FR</td>\n      <td>100</td>\n      <td>45501.0625</td>\n      <td>45501.0625</td>\n      <td>23</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2025</td>\n    </tr>\n  </tbody>\n</table>\n<p>54702 rows × 13 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import (\n    Input, Dense, Dropout, LSTM, GRU, SimpleRNN,\n    Conv1D, MaxPooling1D, GlobalAveragePooling1D,\n    Flatten, Attention, BatchNormalization, Reshape\n)\n\n\nfrom tensorflow.keras.optimizers import Adam","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:07.801585Z","iopub.execute_input":"2025-08-07T17:50:07.801829Z","iopub.status.idle":"2025-08-07T17:50:21.879018Z","shell.execute_reply.started":"2025-08-07T17:50:07.801811Z","shell.execute_reply":"2025-08-07T17:50:21.878244Z"}},"outputs":[{"name":"stderr","text":"2025-08-07 17:50:10.470585: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754589010.636042      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754589010.689833      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"df['DateUTC'] = pd.to_datetime(df['DateUTC'])\ndf.set_index('DateUTC', inplace=True)\n\n# Training: 2019–2024, Testing: 2025\ntrain_df_full = df[df.index.year < 2025]\ntest_df = df[df.index.year == 2025]\n\n# Split validation from end of 2024 (10% of training)\nval_ratio = 0.1\nval_size = int(len(train_df_full) * val_ratio)\nval_df = train_df_full.iloc[-val_size:]\ntrain_df = train_df_full.iloc[:-val_size]\n\n# In thông tin tóm tắt\nprint(\"Train shape:\", train_df.shape)\nprint(\"Validiont shape:\", val_df.shape)\nprint(\"Test shape:\", test_df.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:21.879888Z","iopub.execute_input":"2025-08-07T17:50:21.880484Z","iopub.status.idle":"2025-08-07T17:50:21.912904Z","shell.execute_reply.started":"2025-08-07T17:50:21.880459Z","shell.execute_reply":"2025-08-07T17:50:21.912206Z"}},"outputs":[{"name":"stdout","text":"Train shape: (47348, 12)\nValidiont shape: (5260, 12)\nTest shape: (2160, 12)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def create_sequences(data, seq_len=24, output_len=1):\n    X, y = [], []\n    for i in range(seq_len, len(data) - output_len + 1):\n        X.append(data[i - seq_len:i])\n        y.append(data[i:i + output_len].flatten())\n    return np.array(X), np.array(y)\n\nscaler = MinMaxScaler()\nscaler.fit(df[['Value']])\ntrain_scaled = scaler.transform(train_df[['Value']])\nval_scaled   = scaler.transform(val_df[['Value']])\ntest_scaled  = scaler.transform(test_df[['Value']])\n\nseq_len = 24\noutput_len = 1\n\nX_train, y_train = create_sequences(train_scaled, seq_len, output_len)\nX_val, y_val     = create_sequences(val_scaled, seq_len, output_len)\nX_test, y_test   = create_sequences(test_scaled, seq_len, output_len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:21.913799Z","iopub.execute_input":"2025-08-07T17:50:21.914431Z","iopub.status.idle":"2025-08-07T17:50:22.057587Z","shell.execute_reply.started":"2025-08-07T17:50:21.914413Z","shell.execute_reply":"2025-08-07T17:50:22.056893Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def evaluate_model(y_true, y_pred, scaler=None):\n    y_true = np.squeeze(y_true)\n    y_pred = np.squeeze(y_pred)\n    if scaler:\n        y_true = scaler.inverse_transform(y_true.reshape(-1, 1))\n        y_pred = scaler.inverse_transform(y_pred.reshape(-1, 1))\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mae  = mean_absolute_error(y_true, y_pred)\n    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n    return round(rmse, 2), round(mae, 2), round(mape, 2), y_true, y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:22.058376Z","iopub.execute_input":"2025-08-07T17:50:22.058905Z","iopub.status.idle":"2025-08-07T17:50:22.063197Z","shell.execute_reply.started":"2025-08-07T17:50:22.058878Z","shell.execute_reply":"2025-08-07T17:50:22.062663Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# deep conv with batchnormalization and dropout\ndef build_cnn_lstm_attention_v1(input_shape, output_len=1):\n    inputs = Input(shape=input_shape)\n\n    # Block 1\n    x = Conv1D(64, 3, padding='same', activation='relu')(inputs)\n    x = Conv1D(64, 3, padding='same', activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n\n    # Block 2\n    x = Conv1D(128, 3, padding='same', activation='relu')(x)\n    x = Conv1D(128, 3, padding='same', activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n\n    # Block 3\n    x = Conv1D(256, 3, padding='same', activation='relu')(x)\n    x = Conv1D(256, 3, padding='same', activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n\n    # Block 4\n    x = Conv1D(512, 3, padding='same', activation='relu')(x)\n    x = Conv1D(512, 3, padding='same', activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(2)(x)\n\n    # LSTM + Attention\n    x = LSTM(64, return_sequences=True, implementation=2)(x)\n    x = Attention()([x, x])\n    x = GlobalAveragePooling1D()(x)\n\n    # Dense\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    outputs = Dense(output_len)(x)\n    return Model(inputs, outputs)\n\ndef build_cnn_lstm_attention_v2(input_shape, output_len=1):\n    inputs = Input(shape=input_shape)\n\n    # CNN Blocks\n    x = Conv1D(64, 3, activation='relu', padding='same')(inputs)\n    x = Conv1D(64, 3, activation='relu', padding='same')(x)\n    x = MaxPooling1D(2)(x)\n\n    x = Conv1D(128, 3, activation='relu', padding='same')(x)\n    x = Conv1D(128, 3, activation='relu', padding='same')(x)\n    x = MaxPooling1D(2)(x)\n\n    # Deep LSTM + Attention\n    x = LSTM(64, return_sequences=True)(x)\n    x = LSTM(32, return_sequences=True)(x)\n    x = Attention()([x, x])\n    x = Flatten()(x)\n\n    # Dense\n    x = Dense(64, activation='relu')(x)\n    outputs = Dense(output_len)(x)\n    return Model(inputs, outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:22.063876Z","iopub.execute_input":"2025-08-07T17:50:22.064065Z","iopub.status.idle":"2025-08-07T17:50:22.075831Z","shell.execute_reply.started":"2025-08-07T17:50:22.064050Z","shell.execute_reply":"2025-08-07T17:50:22.075331Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Ablation Study","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers, models, Input\n\n# ✅ Không có CNN (LSTM + Attention)\ndef build_lstm_attention(input_shape, output_len=1):\n    inputs = Input(shape=input_shape)\n    x = layers.LSTM(64, return_sequences=True)(inputs)\n    attn = layers.Attention()([x, x])\n    x = layers.GlobalAveragePooling1D()(attn)\n    outputs = layers.Dense(output_len)(x)\n    model = models.Model(inputs, outputs)\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n    return model\n\n# ✅ Không có LSTM (CNN + Attention)\ndef build_cnn_attention(input_shape, output_len=1):\n    inputs = Input(shape=input_shape)\n    x = layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(inputs)\n    attn = layers.Attention()([x, x])\n    x = layers.GlobalAveragePooling1D()(attn)\n    outputs = layers.Dense(output_len)(x)\n    model = models.Model(inputs, outputs)\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n    return model\n\n# ✅ Không có Attention (CNN + LSTM)\ndef build_cnn_lstm(input_shape, output_len=1):\n    inputs = Input(shape=input_shape)\n    x = layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(inputs)\n    x = layers.LSTM(64)(x)\n    outputs = layers.Dense(output_len)(x)\n    model = models.Model(inputs, outputs)\n    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:22.078918Z","iopub.execute_input":"2025-08-07T17:50:22.079133Z","iopub.status.idle":"2025-08-07T17:50:22.090296Z","shell.execute_reply.started":"2025-08-07T17:50:22.079118Z","shell.execute_reply":"2025-08-07T17:50:22.089787Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def train_model(model, X_train, y_train, X_val, y_val, epochs=20, batch_size=32):\n    model.compile(optimizer=Adam(1e-4), loss='mse')\n    history = model.fit(X_train, y_train, epochs=epochs,\n                        batch_size=batch_size, validation_data=(X_val, y_val), verbose=1)\n    return model, history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:22.090960Z","iopub.execute_input":"2025-08-07T17:50:22.091164Z","iopub.status.idle":"2025-08-07T17:50:22.104115Z","shell.execute_reply.started":"2025-08-07T17:50:22.091149Z","shell.execute_reply":"2025-08-07T17:50:22.103540Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def train_model_group(model_dict, group_name, X_train, y_train, X_val, y_val, X_test, y_test, scaler, output_len=1, epochs=20):\n    results = []\n    preds_dict = {}\n\n    for name, build_fn in model_dict.items():\n        print(f\"\\n[Group: {group_name}] Training {name}...\")\n        model = build_fn(X_train.shape[1:], output_len=output_len)\n        model, _ = train_model(model, X_train, y_train, X_val, y_val, epochs=epochs)\n\n        # Predict\n        y_pred = model.predict(X_test)\n        rmse, mae, mape, y_true, y_pred_inv = evaluate_model(y_test, y_pred, scaler)\n\n        # Save results\n        preds_dict[name] = y_pred_inv\n        results.append({\"Model\": f\"{group_name} - {name}\", \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape})\n\n    return results, preds_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T15:36:36.692481Z","iopub.execute_input":"2025-08-07T15:36:36.692718Z","iopub.status.idle":"2025-08-07T15:36:36.710584Z","shell.execute_reply.started":"2025-08-07T15:36:36.692702Z","shell.execute_reply":"2025-08-07T15:36:36.710090Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Dictionary đầy đủ cho Ablation Study + gốc\nablation_all_models = {\n    \"CNN + LSTM\": build_cnn_lstm,\n    \"CNN + Attention\": build_cnn_attention,\n    \"LSTM + Attention\": build_lstm_attention,\n    \"CNN + LSTM + Attention v1\": build_cnn_lstm_attention_v1,\n    \"CNN + LSTM + Attention v2\": build_cnn_lstm_attention_v2\n}\n\n# Train tất cả models single-step\nablation_results_all, ablation_preds_all = train_model_group(\n    model_dict=ablation_all_models,\n    group_name=\"Ablation-Full-SingleStep\",\n    X_train=X_train,\n    y_train=y_train,\n    X_val=X_val,\n    y_val=y_val,\n    X_test=X_test,\n    y_test=y_test,\n    scaler=scaler,\n    epochs=20\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T15:36:36.711318Z","iopub.execute_input":"2025-08-07T15:36:36.711536Z","iopub.status.idle":"2025-08-07T15:52:58.614202Z","shell.execute_reply.started":"2025-08-07T15:36:36.711518Z","shell.execute_reply":"2025-08-07T15:52:58.613649Z"}},"outputs":[{"name":"stdout","text":"\n[Group: Ablation-Full-SingleStep] Training CNN + LSTM...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754580998.375508      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1754580998.376197      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754581002.386520      84 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 0.0353 - mae: 0.1339 - val_loss: 0.0023 - val_mae: 0.0357\nEpoch 2/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0016 - mae: 0.0310 - val_loss: 9.1430e-04 - val_mae: 0.0228\nEpoch 3/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 8.8928e-04 - mae: 0.0225 - val_loss: 6.7672e-04 - val_mae: 0.0199\nEpoch 4/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 7.4383e-04 - mae: 0.0207 - val_loss: 6.3109e-04 - val_mae: 0.0187\nEpoch 5/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 7.1592e-04 - mae: 0.0205 - val_loss: 7.0406e-04 - val_mae: 0.0194\nEpoch 6/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 6.7491e-04 - mae: 0.0199 - val_loss: 5.7010e-04 - val_mae: 0.0187\nEpoch 7/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 6.6718e-04 - mae: 0.0198 - val_loss: 5.3859e-04 - val_mae: 0.0177\nEpoch 8/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 6.6151e-04 - mae: 0.0197 - val_loss: 5.2316e-04 - val_mae: 0.0176\nEpoch 9/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 6.4390e-04 - mae: 0.0196 - val_loss: 5.1508e-04 - val_mae: 0.0176\nEpoch 10/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 6.3435e-04 - mae: 0.0194 - val_loss: 5.0202e-04 - val_mae: 0.0173\nEpoch 11/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 6.1442e-04 - mae: 0.0191 - val_loss: 5.6505e-04 - val_mae: 0.0178\nEpoch 12/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 6.0453e-04 - mae: 0.0190 - val_loss: 5.4712e-04 - val_mae: 0.0186\nEpoch 13/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.8456e-04 - mae: 0.0187 - val_loss: 4.8971e-04 - val_mae: 0.0168\nEpoch 14/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.8690e-04 - mae: 0.0188 - val_loss: 5.0528e-04 - val_mae: 0.0176\nEpoch 15/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.8357e-04 - mae: 0.0187 - val_loss: 4.6523e-04 - val_mae: 0.0165\nEpoch 16/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.6541e-04 - mae: 0.0184 - val_loss: 5.1249e-04 - val_mae: 0.0171\nEpoch 17/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.5898e-04 - mae: 0.0183 - val_loss: 4.9071e-04 - val_mae: 0.0176\nEpoch 18/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.4159e-04 - mae: 0.0180 - val_loss: 4.3660e-04 - val_mae: 0.0162\nEpoch 19/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.3266e-04 - mae: 0.0179 - val_loss: 4.2503e-04 - val_mae: 0.0160\nEpoch 20/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 5.1884e-04 - mae: 0.0177 - val_loss: 4.5830e-04 - val_mae: 0.0168\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n\n[Group: Ablation-Full-SingleStep] Training CNN + Attention...\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1754581161.426019      83 service.cc:148] XLA service 0x7f572c29ef80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1754581161.427319      83 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1754581161.427342      83 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  75/1479\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3435 - mae: 0.5442","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754581162.495428      83 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.1341 - mae: 0.2902 - val_loss: 0.0228 - val_mae: 0.1258\nEpoch 2/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0249 - mae: 0.1332 - val_loss: 0.0207 - val_mae: 0.1196\nEpoch 3/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0237 - mae: 0.1291 - val_loss: 0.0203 - val_mae: 0.1181\nEpoch 4/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0236 - mae: 0.1284 - val_loss: 0.0201 - val_mae: 0.1174\nEpoch 5/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0232 - mae: 0.1273 - val_loss: 0.0199 - val_mae: 0.1169\nEpoch 6/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0231 - mae: 0.1270 - val_loss: 0.0198 - val_mae: 0.1165\nEpoch 7/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0230 - mae: 0.1270 - val_loss: 0.0195 - val_mae: 0.1164\nEpoch 8/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0227 - mae: 0.1260 - val_loss: 0.0194 - val_mae: 0.1153\nEpoch 9/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0224 - mae: 0.1250 - val_loss: 0.0192 - val_mae: 0.1150\nEpoch 10/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0222 - mae: 0.1245 - val_loss: 0.0191 - val_mae: 0.1140\nEpoch 11/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0220 - mae: 0.1240 - val_loss: 0.0188 - val_mae: 0.1133\nEpoch 12/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0217 - mae: 0.1230 - val_loss: 0.0185 - val_mae: 0.1134\nEpoch 13/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0213 - mae: 0.1220 - val_loss: 0.0182 - val_mae: 0.1116\nEpoch 14/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0209 - mae: 0.1206 - val_loss: 0.0179 - val_mae: 0.1110\nEpoch 15/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0206 - mae: 0.1199 - val_loss: 0.0175 - val_mae: 0.1098\nEpoch 16/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0198 - mae: 0.1173 - val_loss: 0.0173 - val_mae: 0.1083\nEpoch 17/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0198 - mae: 0.1176 - val_loss: 0.0169 - val_mae: 0.1073\nEpoch 18/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0192 - mae: 0.1154 - val_loss: 0.0165 - val_mae: 0.1061\nEpoch 19/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0187 - mae: 0.1141 - val_loss: 0.0162 - val_mae: 0.1047\nEpoch 20/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0183 - mae: 0.1129 - val_loss: 0.0158 - val_mae: 0.1047\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n\n[Group: Ablation-Full-SingleStep] Training LSTM + Attention...\nEpoch 1/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0515 - mae: 0.1776 - val_loss: 0.0207 - val_mae: 0.1170\nEpoch 2/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0178 - mae: 0.1058 - val_loss: 0.0107 - val_mae: 0.0801\nEpoch 3/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0105 - mae: 0.0794 - val_loss: 0.0090 - val_mae: 0.0710\nEpoch 4/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0087 - mae: 0.0720 - val_loss: 0.0076 - val_mae: 0.0647\nEpoch 5/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0070 - mae: 0.0645 - val_loss: 0.0064 - val_mae: 0.0649\nEpoch 6/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0057 - mae: 0.0576 - val_loss: 0.0049 - val_mae: 0.0565\nEpoch 7/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0044 - mae: 0.0497 - val_loss: 0.0033 - val_mae: 0.0427\nEpoch 8/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0033 - mae: 0.0432 - val_loss: 0.0028 - val_mae: 0.0413\nEpoch 9/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0027 - mae: 0.0392 - val_loss: 0.0023 - val_mae: 0.0364\nEpoch 10/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0023 - mae: 0.0357 - val_loss: 0.0019 - val_mae: 0.0329\nEpoch 11/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0019 - mae: 0.0331 - val_loss: 0.0018 - val_mae: 0.0309\nEpoch 12/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0017 - mae: 0.0313 - val_loss: 0.0015 - val_mae: 0.0290\nEpoch 13/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0016 - mae: 0.0296 - val_loss: 0.0015 - val_mae: 0.0280\nEpoch 14/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0014 - mae: 0.0280 - val_loss: 0.0013 - val_mae: 0.0273\nEpoch 15/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0013 - mae: 0.0272 - val_loss: 0.0012 - val_mae: 0.0258\nEpoch 16/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0012 - mae: 0.0259 - val_loss: 0.0010 - val_mae: 0.0237\nEpoch 17/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0011 - mae: 0.0251 - val_loss: 9.6787e-04 - val_mae: 0.0222\nEpoch 18/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0010 - mae: 0.0238 - val_loss: 9.0631e-04 - val_mae: 0.0215\nEpoch 19/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 9.2478e-04 - mae: 0.0226 - val_loss: 8.1773e-04 - val_mae: 0.0215\nEpoch 20/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 8.2304e-04 - mae: 0.0215 - val_loss: 8.4812e-04 - val_mae: 0.0226\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n\n[Group: Ablation-Full-SingleStep] Training CNN + LSTM + Attention v1...\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 12ms/step - loss: 0.0174 - val_loss: 0.0010\nEpoch 2/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0047 - val_loss: 7.2561e-04\nEpoch 3/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0035 - val_loss: 6.4372e-04\nEpoch 4/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0030 - val_loss: 7.0439e-04\nEpoch 5/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0025 - val_loss: 5.7733e-04\nEpoch 6/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0022 - val_loss: 4.2658e-04\nEpoch 7/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0018 - val_loss: 3.5070e-04\nEpoch 8/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0016 - val_loss: 2.5120e-04\nEpoch 9/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0014 - val_loss: 3.6674e-04\nEpoch 10/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0013 - val_loss: 2.9180e-04\nEpoch 11/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0011 - val_loss: 2.7298e-04\nEpoch 12/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0010 - val_loss: 2.6753e-04\nEpoch 13/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0010 - val_loss: 2.4132e-04\nEpoch 14/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 9.5664e-04 - val_loss: 2.7881e-04\nEpoch 15/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 9.2282e-04 - val_loss: 3.1367e-04\nEpoch 16/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 9.1203e-04 - val_loss: 2.8175e-04\nEpoch 17/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 8.6427e-04 - val_loss: 3.5793e-04\nEpoch 18/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 8.8698e-04 - val_loss: 2.6010e-04\nEpoch 19/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 8.7385e-04 - val_loss: 2.4136e-04\nEpoch 20/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 8.3558e-04 - val_loss: 3.2209e-04\n\u001b[1m 1/67\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 373ms/step","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - loss: 0.0372 - val_loss: 0.0012\nEpoch 2/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 9.2941e-04 - val_loss: 7.7147e-04\nEpoch 3/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 5.3140e-04 - val_loss: 4.7118e-04\nEpoch 4/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 4.3619e-04 - val_loss: 3.0119e-04\nEpoch 5/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 3.7770e-04 - val_loss: 5.3819e-04\nEpoch 6/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 3.5690e-04 - val_loss: 3.4642e-04\nEpoch 7/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 3.3450e-04 - val_loss: 2.7083e-04\nEpoch 8/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 2.9699e-04 - val_loss: 2.6006e-04\nEpoch 9/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 2.7913e-04 - val_loss: 2.6295e-04\nEpoch 10/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 2.7946e-04 - val_loss: 2.4124e-04\nEpoch 11/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 2.6314e-04 - val_loss: 2.4442e-04\nEpoch 12/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 2.5747e-04 - val_loss: 2.3492e-04\nEpoch 13/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 2.4280e-04 - val_loss: 2.5568e-04\nEpoch 14/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 2.4981e-04 - val_loss: 2.2957e-04\nEpoch 15/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 2.3193e-04 - val_loss: 2.2021e-04\nEpoch 16/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 2.3339e-04 - val_loss: 2.1602e-04\nEpoch 17/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 2.2020e-04 - val_loss: 2.1278e-04\nEpoch 18/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 2.2165e-04 - val_loss: 2.0850e-04\nEpoch 19/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 2.1454e-04 - val_loss: 2.7737e-04\nEpoch 20/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 2.2072e-04 - val_loss: 2.0831e-04\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from IPython.display import display, HTML\n\n# Make notebook cells wider\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))\nablation_df = pd.DataFrame(ablation_results_all).sort_values(by=\"RMSE\")\n\nfrom IPython.display import display, HTML\n\n# Tăng chiều rộng hiển thị của notebook\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))\n\n# Hiển thị bảng kết quả ablation\ndisplay(ablation_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:44:08.173763Z","iopub.execute_input":"2025-08-07T16:44:08.174611Z","iopub.status.idle":"2025-08-07T16:44:08.190579Z","shell.execute_reply.started":"2025-08-07T16:44:08.174582Z","shell.execute_reply":"2025-08-07T16:44:08.190035Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>.container { width:100% !important; }</style>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>.container { width:100% !important; }</style>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                                               Model    RMSE     MAE  MAPE\n4  Ablation-Full-SingleStep - CNN + LSTM + Attent...  105.77   78.60  1.05\n3  Ablation-Full-SingleStep - CNN + LSTM + Attent...  145.75  110.67  1.42\n0              Ablation-Full-SingleStep - CNN + LSTM  165.85  133.92  1.81\n2        Ablation-Full-SingleStep - LSTM + Attention  218.26  168.34  2.24\n1         Ablation-Full-SingleStep - CNN + Attention  886.41  748.10  9.85","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>Ablation-Full-SingleStep - CNN + LSTM + Attent...</td>\n      <td>105.77</td>\n      <td>78.60</td>\n      <td>1.05</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ablation-Full-SingleStep - CNN + LSTM + Attent...</td>\n      <td>145.75</td>\n      <td>110.67</td>\n      <td>1.42</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Ablation-Full-SingleStep - CNN + LSTM</td>\n      <td>165.85</td>\n      <td>133.92</td>\n      <td>1.81</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ablation-Full-SingleStep - LSTM + Attention</td>\n      <td>218.26</td>\n      <td>168.34</td>\n      <td>2.24</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ablation-Full-SingleStep - CNN + Attention</td>\n      <td>886.41</td>\n      <td>748.10</td>\n      <td>9.85</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"### multi step for ablation model","metadata":{}},{"cell_type":"code","source":"def train_multistep_model_group(\n    model_dict, group_name,\n    X_train, y_train,\n    X_val, y_val,\n    X_test, y_test,\n    scaler,\n    output_len=24,\n    epochs=20\n):\n    results = []\n    preds_dict = {}\n\n    for name, build_fn in model_dict.items():\n        print(f\"\\n[Group: {group_name}] Training {name}...\")\n\n        model = build_fn(X_train.shape[1:], output_len=output_len)\n        model, _ = train_model(model, X_train, y_train, X_val, y_val, epochs=epochs)\n\n        y_pred = model.predict(X_test)\n\n        if y_pred.shape != y_test.shape:\n            print(f\"Warning: Shape mismatch! y_test: {y_test.shape}, y_pred: {y_pred.shape}\")\n            continue\n\n        y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n        y_pred_inv = scaler.inverse_transform(y_pred.reshape(-1, 1)).reshape(y_pred.shape)\n\n        y_test_flat = y_test_inv.flatten()\n        y_pred_flat = y_pred_inv.flatten()\n\n        rmse = np.sqrt(mean_squared_error(y_test_flat, y_pred_flat))\n        mae = mean_absolute_error(y_test_flat, y_pred_flat)\n        mape = np.mean(np.abs((y_test_flat - y_pred_flat) / (y_test_flat + 1e-8))) * 100\n\n        results.append({\n            \"Model\": f\"{group_name} - {name}\",\n            \"RMSE\": round(rmse, 2),\n            \"MAE\": round(mae, 2),\n            \"MAPE\": round(mape, 2)\n        })\n\n        preds_dict[name] = y_pred_inv\n\n    return results, preds_dict, y_test_inv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:22.106188Z","iopub.execute_input":"2025-08-07T17:50:22.106394Z","iopub.status.idle":"2025-08-07T17:50:22.115940Z","shell.execute_reply.started":"2025-08-07T17:50:22.106379Z","shell.execute_reply":"2025-08-07T17:50:22.115445Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Tạo sequence cho multistep\nseq_len = 24\noutput_len = 24\n\nX_train, y_train = create_sequences(train_scaled, seq_len=seq_len, output_len=output_len)\nX_val, y_val     = create_sequences(val_scaled, seq_len=seq_len, output_len=output_len)\nX_test, y_test   = create_sequences(test_scaled, seq_len=seq_len, output_len=output_len)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:22.116664Z","iopub.execute_input":"2025-08-07T17:50:22.117381Z","iopub.status.idle":"2025-08-07T17:50:22.258306Z","shell.execute_reply.started":"2025-08-07T17:50:22.117362Z","shell.execute_reply":"2025-08-07T17:50:22.257779Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Tập hợp các model ablation\nablation_multistep_models = {\n    \"LSTM + Attention\": build_lstm_attention,\n    \"CNN + Attention\": build_cnn_attention,\n    \"CNN + LSTM\": build_cnn_lstm,\n    \"CNN + LSTM + Attention v1\": build_cnn_lstm_attention_v1,\n    \"CNN + LSTM + Attention v2\": build_cnn_lstm_attention_v2\n}\n\n# Train và đánh giá\nablation_ms_results, ablation_ms_preds, y_test_inv_ablation = train_multistep_model_group(\n    model_dict=ablation_multistep_models,\n    group_name=\"Ablation-Multistep\",\n    X_train=X_train, y_train=y_train,\n    X_val=X_val, y_val=y_val,\n    X_test=X_test, y_test=y_test,\n    scaler=scaler,\n    output_len=24,\n    epochs=20\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T17:50:22.259459Z","iopub.execute_input":"2025-08-07T17:50:22.259712Z","iopub.status.idle":"2025-08-07T18:07:00.585057Z","shell.execute_reply.started":"2025-08-07T17:50:22.259683Z","shell.execute_reply":"2025-08-07T18:07:00.584528Z"}},"outputs":[{"name":"stdout","text":"\n[Group: Ablation-Multistep] Training LSTM + Attention...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754589023.056707      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1754589023.057508      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754589026.770304      81 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0782 - mae: 0.2168 - val_loss: 0.0232 - val_mae: 0.1257\nEpoch 2/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0225 - mae: 0.1212 - val_loss: 0.0167 - val_mae: 0.1033\nEpoch 3/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0168 - mae: 0.1006 - val_loss: 0.0154 - val_mae: 0.0983\nEpoch 4/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0156 - mae: 0.0963 - val_loss: 0.0147 - val_mae: 0.0960\nEpoch 5/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0145 - mae: 0.0922 - val_loss: 0.0135 - val_mae: 0.0908\nEpoch 6/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0137 - mae: 0.0886 - val_loss: 0.0125 - val_mae: 0.0859\nEpoch 7/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0128 - mae: 0.0851 - val_loss: 0.0120 - val_mae: 0.0837\nEpoch 8/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0122 - mae: 0.0826 - val_loss: 0.0117 - val_mae: 0.0815\nEpoch 9/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0117 - mae: 0.0806 - val_loss: 0.0112 - val_mae: 0.0800\nEpoch 10/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0111 - mae: 0.0785 - val_loss: 0.0110 - val_mae: 0.0789\nEpoch 11/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0108 - mae: 0.0772 - val_loss: 0.0112 - val_mae: 0.0797\nEpoch 12/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0106 - mae: 0.0762 - val_loss: 0.0109 - val_mae: 0.0784\nEpoch 13/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0105 - mae: 0.0758 - val_loss: 0.0106 - val_mae: 0.0770\nEpoch 14/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0103 - mae: 0.0751 - val_loss: 0.0105 - val_mae: 0.0770\nEpoch 15/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0104 - mae: 0.0750 - val_loss: 0.0102 - val_mae: 0.0751\nEpoch 16/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0100 - mae: 0.0734 - val_loss: 0.0106 - val_mae: 0.0772\nEpoch 17/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0100 - mae: 0.0733 - val_loss: 0.0099 - val_mae: 0.0736\nEpoch 18/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0098 - mae: 0.0725 - val_loss: 0.0113 - val_mae: 0.0804\nEpoch 19/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0723 - val_loss: 0.0096 - val_mae: 0.0724\nEpoch 20/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 0.0095 - mae: 0.0712 - val_loss: 0.0113 - val_mae: 0.0798\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n\n[Group: Ablation-Multistep] Training CNN + Attention...\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1754589174.646796      81 service.cc:148] XLA service 0x3cc69690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1754589174.647623      81 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1754589174.647641      81 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  71/1479\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.2648 - mae: 0.4652  ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754589176.290447      81 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.1524 - mae: 0.3229 - val_loss: 0.0258 - val_mae: 0.1347\nEpoch 2/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0290 - mae: 0.1443 - val_loss: 0.0252 - val_mae: 0.1333\nEpoch 3/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0286 - mae: 0.1429 - val_loss: 0.0249 - val_mae: 0.1321\nEpoch 4/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0284 - mae: 0.1419 - val_loss: 0.0248 - val_mae: 0.1319\nEpoch 5/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0283 - mae: 0.1415 - val_loss: 0.0246 - val_mae: 0.1312\nEpoch 6/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0283 - mae: 0.1410 - val_loss: 0.0246 - val_mae: 0.1311\nEpoch 7/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0283 - mae: 0.1411 - val_loss: 0.0246 - val_mae: 0.1312\nEpoch 8/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0282 - mae: 0.1409 - val_loss: 0.0245 - val_mae: 0.1310\nEpoch 9/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0283 - mae: 0.1411 - val_loss: 0.0245 - val_mae: 0.1308\nEpoch 10/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0280 - mae: 0.1404 - val_loss: 0.0243 - val_mae: 0.1305\nEpoch 11/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0280 - mae: 0.1405 - val_loss: 0.0243 - val_mae: 0.1305\nEpoch 12/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0280 - mae: 0.1404 - val_loss: 0.0243 - val_mae: 0.1304\nEpoch 13/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0280 - mae: 0.1403 - val_loss: 0.0241 - val_mae: 0.1299\nEpoch 14/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0277 - mae: 0.1396 - val_loss: 0.0241 - val_mae: 0.1299\nEpoch 15/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0276 - mae: 0.1394 - val_loss: 0.0240 - val_mae: 0.1297\nEpoch 16/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0275 - mae: 0.1392 - val_loss: 0.0239 - val_mae: 0.1293\nEpoch 17/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0275 - mae: 0.1389 - val_loss: 0.0238 - val_mae: 0.1292\nEpoch 18/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0272 - mae: 0.1384 - val_loss: 0.0238 - val_mae: 0.1289\nEpoch 19/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0272 - mae: 0.1385 - val_loss: 0.0237 - val_mae: 0.1287\nEpoch 20/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0271 - mae: 0.1379 - val_loss: 0.0236 - val_mae: 0.1286\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n\n[Group: Ablation-Multistep] Training CNN + LSTM...\nEpoch 1/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 0.0596 - mae: 0.1840 - val_loss: 0.0152 - val_mae: 0.0975\nEpoch 2/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0148 - mae: 0.0937 - val_loss: 0.0131 - val_mae: 0.0889\nEpoch 3/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0125 - mae: 0.0841 - val_loss: 0.0115 - val_mae: 0.0813\nEpoch 4/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0110 - mae: 0.0773 - val_loss: 0.0106 - val_mae: 0.0775\nEpoch 5/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0103 - mae: 0.0742 - val_loss: 0.0098 - val_mae: 0.0738\nEpoch 6/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0096 - mae: 0.0716 - val_loss: 0.0099 - val_mae: 0.0746\nEpoch 7/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0093 - mae: 0.0704 - val_loss: 0.0090 - val_mae: 0.0698\nEpoch 8/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0088 - mae: 0.0682 - val_loss: 0.0086 - val_mae: 0.0683\nEpoch 9/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0085 - mae: 0.0669 - val_loss: 0.0083 - val_mae: 0.0664\nEpoch 10/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0081 - mae: 0.0651 - val_loss: 0.0086 - val_mae: 0.0672\nEpoch 11/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0078 - mae: 0.0637 - val_loss: 0.0095 - val_mae: 0.0715\nEpoch 12/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0077 - mae: 0.0629 - val_loss: 0.0077 - val_mae: 0.0635\nEpoch 13/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0076 - mae: 0.0623 - val_loss: 0.0075 - val_mae: 0.0628\nEpoch 14/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0074 - mae: 0.0615 - val_loss: 0.0079 - val_mae: 0.0637\nEpoch 15/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0073 - mae: 0.0610 - val_loss: 0.0079 - val_mae: 0.0641\nEpoch 16/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0072 - mae: 0.0605 - val_loss: 0.0082 - val_mae: 0.0651\nEpoch 17/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0071 - mae: 0.0600 - val_loss: 0.0077 - val_mae: 0.0627\nEpoch 18/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0070 - mae: 0.0595 - val_loss: 0.0074 - val_mae: 0.0615\nEpoch 19/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0068 - mae: 0.0585 - val_loss: 0.0089 - val_mae: 0.0670\nEpoch 20/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.0067 - mae: 0.0584 - val_loss: 0.0070 - val_mae: 0.0603\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n\n[Group: Ablation-Multistep] Training CNN + LSTM + Attention v1...\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 13ms/step - loss: 0.0770 - val_loss: 0.0078\nEpoch 2/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0185 - val_loss: 0.0065\nEpoch 3/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0130 - val_loss: 0.0060\nEpoch 4/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0107 - val_loss: 0.0064\nEpoch 5/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0091 - val_loss: 0.0059\nEpoch 6/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0080 - val_loss: 0.0055\nEpoch 7/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0074 - val_loss: 0.0058\nEpoch 8/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0068 - val_loss: 0.0052\nEpoch 9/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0065 - val_loss: 0.0049\nEpoch 10/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0061 - val_loss: 0.0054\nEpoch 11/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0059 - val_loss: 0.0048\nEpoch 12/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0057 - val_loss: 0.0055\nEpoch 13/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0057 - val_loss: 0.0048\nEpoch 14/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0054 - val_loss: 0.0048\nEpoch 15/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0053 - val_loss: 0.0049\nEpoch 16/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0051 - val_loss: 0.0044\nEpoch 17/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0050 - val_loss: 0.0045\nEpoch 18/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0049 - val_loss: 0.0048\nEpoch 19/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0049 - val_loss: 0.0043\nEpoch 20/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 0.0048 - val_loss: 0.0045\n\u001b[1m 1/67\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 382ms/step","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n\n[Group: Ablation-Multistep] Training CNN + LSTM + Attention v2...\nEpoch 1/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - loss: 0.0666 - val_loss: 0.0137\nEpoch 2/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0124 - val_loss: 0.0099\nEpoch 3/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0088 - val_loss: 0.0081\nEpoch 4/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0078 - val_loss: 0.0080\nEpoch 5/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0072 - val_loss: 0.0073\nEpoch 6/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0068 - val_loss: 0.0070\nEpoch 7/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0066 - val_loss: 0.0071\nEpoch 8/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0064 - val_loss: 0.0071\nEpoch 9/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0061 - val_loss: 0.0066\nEpoch 10/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0060 - val_loss: 0.0074\nEpoch 11/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0058 - val_loss: 0.0064\nEpoch 12/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0056 - val_loss: 0.0062\nEpoch 13/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0056 - val_loss: 0.0060\nEpoch 14/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0053 - val_loss: 0.0065\nEpoch 15/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0052 - val_loss: 0.0059\nEpoch 16/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0052 - val_loss: 0.0056\nEpoch 17/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0049 - val_loss: 0.0060\nEpoch 18/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0050 - val_loss: 0.0060\nEpoch 19/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0049 - val_loss: 0.0064\nEpoch 20/20\n\u001b[1m1479/1479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0048 - val_loss: 0.0063\n\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import pandas as pd\npd.DataFrame(ablation_ms_results).sort_values(\"RMSE\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T18:07:00.585812Z","iopub.execute_input":"2025-08-07T18:07:00.586054Z","iopub.status.idle":"2025-08-07T18:07:00.596306Z","shell.execute_reply.started":"2025-08-07T18:07:00.586039Z","shell.execute_reply":"2025-08-07T18:07:00.595762Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                                            Model     RMSE     MAE   MAPE\n3  Ablation-Multistep - CNN + LSTM + Attention v1   449.40  325.97   4.25\n4  Ablation-Multistep - CNN + LSTM + Attention v2   528.25  380.18   5.11\n2                 Ablation-Multistep - CNN + LSTM   577.13  415.97   5.43\n0           Ablation-Multistep - LSTM + Attention   724.29  548.27   7.45\n1            Ablation-Multistep - CNN + Attention  1065.73  885.55  11.69","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>Ablation-Multistep - CNN + LSTM + Attention v1</td>\n      <td>449.40</td>\n      <td>325.97</td>\n      <td>4.25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ablation-Multistep - CNN + LSTM + Attention v2</td>\n      <td>528.25</td>\n      <td>380.18</td>\n      <td>5.11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ablation-Multistep - CNN + LSTM</td>\n      <td>577.13</td>\n      <td>415.97</td>\n      <td>5.43</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Ablation-Multistep - LSTM + Attention</td>\n      <td>724.29</td>\n      <td>548.27</td>\n      <td>7.45</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ablation-Multistep - CNN + Attention</td>\n      <td>1065.73</td>\n      <td>885.55</td>\n      <td>11.69</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Transfer learning","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# Feature columns + target\nfeatures = ['hour', 'dayofweek', 'month', 'year']\ntarget = 'Value'\n\n# Scale dựa trên France\nscaler_transfer = MinMaxScaler()\n\n# Scale riêng target\ntarget_fr_scaled = scaler_transfer.fit_transform(df_fr[[target]])\ntarget_at_scaled = scaler_transfer.transform(df[[target]])\n\n# Nối lại với features (không scale)\nfr_scaled = np.concatenate([df_fr[features].values, target_fr_scaled], axis=1)\nat_scaled = np.concatenate([df[features].values, target_at_scaled], axis=1)\n\n\ndef create_sequences(data, seq_len=24, output_len=1):\n    X, y = [], []\n    for i in range(len(data) - seq_len - output_len + 1):\n        X.append(data[i:i+seq_len, :-1])  # chỉ features\n        y.append(data[i+seq_len:i+seq_len+output_len, -1])  # chỉ target\n    return np.array(X), np.array(y)\n\n# France = Train\nX_train, y_train = create_sequences(fr_scaled, seq_len=24, output_len=1)\n\n# AT = Test (no need to split val)\nX_test, y_test = create_sequences(at_scaled, seq_len=24, output_len=1)\n\n# Split France thành train/val (80/20)\nsplit_idx = int(0.8 * len(X_train))\nX_val, y_val = X_train[split_idx:], y_train[split_idx:]\nX_train, y_train = X_train[:split_idx], y_train[:split_idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:22:12.110672Z","iopub.execute_input":"2025-08-07T16:22:12.111222Z","iopub.status.idle":"2025-08-07T16:22:12.348615Z","shell.execute_reply.started":"2025-08-07T16:22:12.111192Z","shell.execute_reply":"2025-08-07T16:22:12.347984Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# 2 mô hình chính\ntransfer_models = {\n    \"CNN-LSTM-Attention v1 (Transfer)\": build_cnn_lstm_attention_v1,\n    \"CNN-LSTM-Attention v2 (Transfer)\": build_cnn_lstm_attention_v2\n}\n\n# Train với train: France, test: Austria\ntransfer_results, transfer_preds = train_model_group(\n    model_dict=transfer_models,\n    group_name=\"Transfer\",\n    X_train=X_train, y_train=y_train,\n    X_val=X_val, y_val=y_val,\n    X_test=X_test, y_test=y_test,\n    scaler=scaler_transfer,\n    output_len=1,\n    epochs=20\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:22:13.008959Z","iopub.execute_input":"2025-08-07T16:22:13.009208Z","iopub.status.idle":"2025-08-07T16:32:07.655147Z","shell.execute_reply.started":"2025-08-07T16:22:13.009190Z","shell.execute_reply":"2025-08-07T16:32:07.654507Z"}},"outputs":[{"name":"stdout","text":"\n[Group: Transfer] Training CNN-LSTM-Attention v1 (Transfer)...\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 13ms/step - loss: 0.0210 - val_loss: 0.0661\nEpoch 2/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0147 - val_loss: 0.0991\nEpoch 3/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0126 - val_loss: 0.0821\nEpoch 4/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0114 - val_loss: 0.0588\nEpoch 5/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - loss: 0.0107 - val_loss: 0.2063\nEpoch 6/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0103 - val_loss: 0.0561\nEpoch 7/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0095 - val_loss: 0.0197\nEpoch 8/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0094 - val_loss: 0.0410\nEpoch 9/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0094 - val_loss: 0.0476\nEpoch 10/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0089 - val_loss: 0.1572\nEpoch 11/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - loss: 0.0089 - val_loss: 0.1100\nEpoch 12/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0084 - val_loss: 0.1481\nEpoch 13/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0085 - val_loss: 0.0510\nEpoch 14/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0081 - val_loss: 0.1495\nEpoch 15/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - loss: 0.0083 - val_loss: 0.0424\nEpoch 16/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0080 - val_loss: 0.1030\nEpoch 17/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 12ms/step - loss: 0.0080 - val_loss: 0.1158\nEpoch 18/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - loss: 0.0079 - val_loss: 0.0328\nEpoch 19/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - loss: 0.0080 - val_loss: 0.0738\nEpoch 20/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 13ms/step - loss: 0.0077 - val_loss: 0.1450\n\u001b[1m   1/1711\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:49\u001b[0m 380ms/step","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1711/1711\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step\n\n[Group: Transfer] Training CNN-LSTM-Attention v2 (Transfer)...\nEpoch 1/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - loss: 0.0361 - val_loss: 0.0315\nEpoch 2/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0354 - val_loss: 0.0316\nEpoch 3/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0356 - val_loss: 0.0318\nEpoch 4/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - loss: 0.0354 - val_loss: 0.0317\nEpoch 5/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0357 - val_loss: 0.0317\nEpoch 6/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0352 - val_loss: 0.0317\nEpoch 7/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - loss: 0.0357 - val_loss: 0.0317\nEpoch 8/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0353 - val_loss: 0.0320\nEpoch 9/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 9ms/step - loss: 0.0356 - val_loss: 0.0316\nEpoch 10/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0357 - val_loss: 0.0316\nEpoch 11/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0356 - val_loss: 0.0315\nEpoch 12/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - loss: 0.0353 - val_loss: 0.0315\nEpoch 13/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0355 - val_loss: 0.0316\nEpoch 14/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0352 - val_loss: 0.0315\nEpoch 15/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0355 - val_loss: 0.0316\nEpoch 16/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0354 - val_loss: 0.0316\nEpoch 17/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0351 - val_loss: 0.0321\nEpoch 18/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0355 - val_loss: 0.0316\nEpoch 19/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0351 - val_loss: 0.0315\nEpoch 20/20\n\u001b[1m1367/1367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0354 - val_loss: 0.0316\n\u001b[1m1711/1711\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"transfer_df = pd.DataFrame(transfer_results).sort_values(by=\"RMSE\")\ndisplay(transfer_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:32:07.656270Z","iopub.execute_input":"2025-08-07T16:32:07.656515Z","iopub.status.idle":"2025-08-07T16:32:07.667100Z","shell.execute_reply.started":"2025-08-07T16:32:07.656486Z","shell.execute_reply":"2025-08-07T16:32:07.666403Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                         Model      RMSE       MAE    MAPE\n1  Transfer - CNN-LSTM-Attention v2 (Transfer)  44815.70  44795.87  670.65\n0  Transfer - CNN-LSTM-Attention v1 (Transfer)  58545.58  57454.66  863.52","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE</th>\n      <th>MAE</th>\n      <th>MAPE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Transfer - CNN-LSTM-Attention v2 (Transfer)</td>\n      <td>44815.70</td>\n      <td>44795.87</td>\n      <td>670.65</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Transfer - CNN-LSTM-Attention v1 (Transfer)</td>\n      <td>58545.58</td>\n      <td>57454.66</td>\n      <td>863.52</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import ttest_rel\n\n# Lấy prediction ra\ny_true = scaler_transfer.inverse_transform(\n    np.concatenate([y_test, np.zeros((len(y_test), X_test.shape[2]-1))], axis=1)\n)[:, 0]\n\ny_pred_v1 = transfer_preds[\"CNN-LSTM-Attention v1 (Transfer)\"].flatten()\ny_pred_v2 = transfer_preds[\"CNN-LSTM-Attention v2 (Transfer)\"].flatten()\n\n# RMSE / MAPE theo sample\nrmse_v1 = np.abs(y_true - y_pred_v1)\nrmse_v2 = np.abs(y_true - y_pred_v2)\n\nmape_v1 = np.abs((y_true - y_pred_v1) / (y_true + 1e-8)) * 100\nmape_v2 = np.abs((y_true - y_pred_v2) / (y_true + 1e-8)) * 100\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:32:07.667725Z","iopub.execute_input":"2025-08-07T16:32:07.667955Z","iopub.status.idle":"2025-08-07T16:32:07.701905Z","shell.execute_reply.started":"2025-08-07T16:32:07.667939Z","shell.execute_reply":"2025-08-07T16:32:07.701341Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"print(\"Paired t-test for RMSE:\")\nt_rmse, p_rmse = ttest_rel(rmse_v1, rmse_v2)\nprint(f\"t-statistic: {t_rmse:.4f}, p-value: {p_rmse:.4f}\")\n\nprint(\"\\nPaired t-test for MAPE:\")\nt_mape, p_mape = ttest_rel(mape_v1, mape_v2)\nprint(f\"t-statistic: {t_mape:.4f}, p-value: {p_mape:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:32:07.703523Z","iopub.execute_input":"2025-08-07T16:32:07.703943Z","iopub.status.idle":"2025-08-07T16:32:07.712190Z","shell.execute_reply.started":"2025-08-07T16:32:07.703924Z","shell.execute_reply":"2025-08-07T16:32:07.711301Z"}},"outputs":[{"name":"stdout","text":"Paired t-test for RMSE:\nt-statistic: 269.5903, p-value: 0.0000\n\nPaired t-test for MAPE:\nt-statistic: 263.5505, p-value: 0.0000\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"pip install arch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:32:07.713312Z","iopub.execute_input":"2025-08-07T16:32:07.713590Z","iopub.status.idle":"2025-08-07T16:32:15.454642Z","shell.execute_reply.started":"2025-08-07T16:32:07.713559Z","shell.execute_reply":"2025-08-07T16:32:15.453865Z"}},"outputs":[{"name":"stdout","text":"Collecting arch\n  Downloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from arch) (1.26.4)\nRequirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from arch) (1.15.3)\nRequirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.11/dist-packages (from arch) (2.2.3)\nRequirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.11/dist-packages (from arch) (0.14.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->arch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->arch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->arch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->arch) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->arch) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->arch) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4->arch) (2025.2)\nRequirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.12->arch) (1.0.1)\nRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.12->arch) (25.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4->arch) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.3->arch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.3->arch) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.3->arch) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.3->arch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.3->arch) (2024.2.0)\nDownloading arch-7.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (985 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m985.3/985.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: arch\nSuccessfully installed arch-7.2.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"dm_stat_rmse, p_dm_rmse = dm_test(rmse_v1, rmse_v2, h=1, crit=\"MSE\")\nprint(f\"DM Test (RMSE): stat = {dm_stat_rmse:.4f}, p = {p_dm_rmse:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:54:02.397745Z","iopub.execute_input":"2025-08-07T16:54:02.398077Z","iopub.status.idle":"2025-08-07T16:54:02.404211Z","shell.execute_reply.started":"2025-08-07T16:54:02.398053Z","shell.execute_reply":"2025-08-07T16:54:02.403470Z"}},"outputs":[{"name":"stdout","text":"DM Test (RMSE): stat = 294.5308, p = 0.0000\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}